{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory text analysis tutorial\n",
    "\n",
    "This notebook contains code for the tutorial on \"Exploratory text analysis for computational social science.\"\n",
    "The following sections will be used throughout the tutorial.\n",
    "\n",
    "1. [Word frequency](#Word-frequency)\n",
    "2. [Topic modeling]()\n",
    "3. [Word embeddings]()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word frequency\n",
    "\n",
    "First step in exploration: which words occur more frequently in one data set versus another?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>Alex Jones Vindicated in \"Pizzagate\" Controversy</td>\n",
       "      <td>\"Alex Jones, purveyor of the independent inves...</td>\n",
       "      <td>biz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THE BIG DATA CONSPIRACY</td>\n",
       "      <td>so that in the no so far future can institute ...</td>\n",
       "      <td>biz</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>California Surprisingly Lenient on Auto Emissi...</td>\n",
       "      <td>Setting Up Face-Off With Trump \"California's c...</td>\n",
       "      <td>biz</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Mexicans Are Chomping at the Bit to Stop NAFTA...</td>\n",
       "      <td>Mexico has been unfairly gaining from NAFTA as...</td>\n",
       "      <td>biz</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>Breaking News: Snapchat to purchase Twitter fo...</td>\n",
       "      <td>Yahoo and AOL could be extremely popular over ...</td>\n",
       "      <td>biz</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "69    Alex Jones Vindicated in \"Pizzagate\" Controversy   \n",
       "4                              THE BIG DATA CONSPIRACY   \n",
       "202  California Surprisingly Lenient on Auto Emissi...   \n",
       "138  Mexicans Are Chomping at the Bit to Stop NAFTA...   \n",
       "181  Breaking News: Snapchat to purchase Twitter fo...   \n",
       "\n",
       "                                                  text topic id  \n",
       "69   \"Alex Jones, purveyor of the independent inves...   biz  1  \n",
       "4    so that in the no so far future can institute ...   biz  2  \n",
       "202  Setting Up Face-Off With Trump \"California's c...   biz  3  \n",
       "138  Mexico has been unfairly gaining from NAFTA as...   biz  4  \n",
       "181  Yahoo and AOL could be extremely popular over ...   biz  5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alex Jones Apologizes for Promoting 'Pizzagate...</td>\n",
       "      <td>Alex Jones  a prominent conspiracy theorist an...</td>\n",
       "      <td>biz</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Banks and Tech Firms Battle Over Something Aki...</td>\n",
       "      <td>The big banks and Silicon Valley are waging an...</td>\n",
       "      <td>biz</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>California Upholds Auto Emissions Standards</td>\n",
       "      <td>Setting Up Face-Off With Trump  \"California's ...</td>\n",
       "      <td>biz</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Renegotiate Nafta? Mexicans Say Get On With It</td>\n",
       "      <td>For more than two decades  free trade has been...</td>\n",
       "      <td>biz</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>Snapchat 'will be bigger than Twitter</td>\n",
       "      <td>Yahoo and AOL with advertisers'  \"Snapchat cou...</td>\n",
       "      <td>biz</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 title  \\\n",
       "0    Alex Jones Apologizes for Promoting 'Pizzagate...   \n",
       "11   Banks and Tech Firms Battle Over Something Aki...   \n",
       "4          California Upholds Auto Emissions Standards   \n",
       "171     Renegotiate Nafta? Mexicans Say Get On With It   \n",
       "168              Snapchat 'will be bigger than Twitter   \n",
       "\n",
       "                                                  text topic id  \n",
       "0    Alex Jones  a prominent conspiracy theorist an...   biz  1  \n",
       "11   The big banks and Silicon Valley are waging an...   biz  2  \n",
       "4    Setting Up Face-Off With Trump  \"California's ...   biz  3  \n",
       "171  For more than two decades  free trade has been...   biz  4  \n",
       "168  Yahoo and AOL with advertisers'  \"Snapchat cou...   biz  5  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n"
     ]
    }
   ],
   "source": [
    "## small fake news dataset\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "FILE_TOPIC_MATCHER = re.compile('^[a-z]+(?=[0-9])')\n",
    "FILE_ID_MATCHER = re.compile('(?<=[a-z])[0-9]+')\n",
    "FILE_ENDING_MATCHER = re.compile('\\.(fake|legit)\\.txt')\n",
    "def process_file(text_file):\n",
    "    # get file topic/ID\n",
    "    text_file_clean = FILE_ENDING_MATCHER.sub('', os.path.basename(text_file))\n",
    "#     print(text_file_clean)\n",
    "    article_topic = FILE_TOPIC_MATCHER.search(text_file_clean).group(0)\n",
    "    article_id = int(FILE_ID_MATCHER.search(text_file_clean).group(0))\n",
    "    text_file_lines = open(text_file, 'r').readlines()\n",
    "    text_file_lines = list(map(lambda x: x.strip(), text_file_lines))\n",
    "    article_title = text_file_lines[0]\n",
    "    article_text = text_file_lines[-1]\n",
    "    article_data = pd.Series([article_title, article_text, article_topic, article_id], \n",
    "                             index=['title', 'text', 'topic', 'id'])\n",
    "    return article_data\n",
    "\n",
    "def load_all_data(data_dir):\n",
    "    data_files = list(map(lambda x: os.path.join(data_dir, x), os.listdir(data_dir)))\n",
    "    data = pd.concat(list(map(lambda x: process_file(x), data_files)), axis=1).transpose()\n",
    "    data.sort_values(['topic', 'id'], inplace=True, ascending=True)\n",
    "    return data\n",
    "\n",
    "fake_news_data_dir = 'data/fakeNewsDatasets/fakeNewsDataset/fake/'\n",
    "real_news_data_dir = 'data/fakeNewsDatasets/fakeNewsDataset/legit/'\n",
    "fake_news_data = load_all_data(fake_news_data_dir)\n",
    "real_news_data = load_all_data(real_news_data_dir)\n",
    "## save to combined files!!\n",
    "fake_news_data.to_csv('data/fakeNewsDatasets/fake_news_small.tsv', sep='\\t', index=False)\n",
    "real_news_data.to_csv('data/fakeNewsDatasets/real_news_small.tsv', sep='\\t', index=False)\n",
    "display(fake_news_data.head())\n",
    "display(real_news_data.head())\n",
    "print(fake_news_data.shape[0])\n",
    "# print(fake_news_data[5])\n",
    "# print(real_news_data[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/CORE_tutorial/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "# word frequency\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from stop_words import get_stop_words\n",
    "en_stops = get_stop_words('en')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
    "                     tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
    "                     ngram_range=(1,1))\n",
    "# get vocab for all data\n",
    "combined_txt = fake_news_data.loc[:, 'text'].append(real_news_data.loc[:, 'text'])\n",
    "combined_txt_dtm = cv.fit_transform(combined_txt)\n",
    "sorted_vocab = list(sorted(cv.vocabulary_.keys(), key=cv.vocabulary_.get))\n",
    "# get separate DTM for each news data\n",
    "cv = CountVectorizer(min_df=0.001, max_df=0.75, tokenizer=tokenizer.tokenize, stop_words=en_stops, vocabulary=vocab)\n",
    "fake_news_dtm = cv.fit_transform(fake_news_data.loc[:, 'text'].values)\n",
    "real_news_dtm = cv.fit_transform(real_news_data.loc[:, 'text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\".          851\n",
      "249          384\n",
      "!            343\n",
      "100th        278\n",
      "'            182\n",
      "20th         161\n",
      "24           113\n",
      "3bn           90\n",
      "2010          89\n",
      "allows        86\n",
      "650m          76\n",
      "biased        61\n",
      "adult         56\n",
      "2013          54\n",
      "2018          53\n",
      "2007          53\n",
      "10th          49\n",
      "119           48\n",
      "admission     43\n",
      "46            43\n",
      "dtype: int64\n",
      "249         451\n",
      "'           344\n",
      "!           339\n",
      "100th       323\n",
      "!\".         314\n",
      "3bn         148\n",
      "20th         87\n",
      ")-           64\n",
      "19th         53\n",
      "biased       52\n",
      "2010         51\n",
      "24           47\n",
      "allows       47\n",
      "75           45\n",
      "athlete      41\n",
      "2018         41\n",
      "),           38\n",
      "actual       38\n",
      "bathroom     38\n",
      "brash        37\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "## top words\n",
    "import numpy as np\n",
    "fake_news_dtm_top_words = pd.Series(np.array(fake_news_dtm.sum(axis=0))[0], index=sorted_vocab).sort_values(ascending=False)\n",
    "real_news_dtm_top_words = pd.Series(np.array(real_news_dtm.sum(axis=0))[0], index=sorted_vocab).sort_values(ascending=False)\n",
    "print(fake_news_dtm_top_words.head(20))\n",
    "print(real_news_dtm_top_words.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = biz\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       ",            106\n",
       "'             41\n",
       "\"             34\n",
       "s             33\n",
       "-             28\n",
       "will          28\n",
       "uk            21\n",
       "said          19\n",
       "$             14\n",
       "trump         13\n",
       "eu            13\n",
       "deal          13\n",
       ".\"            13\n",
       "company       11\n",
       "many          10\n",
       "companies     10\n",
       "european      10\n",
       "now            9\n",
       "may            8\n",
       "jobs           8\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'            73\n",
       "s            65\n",
       "-            58\n",
       "\"            50\n",
       "said         44\n",
       "$            26\n",
       "will         18\n",
       "us           16\n",
       "1            16\n",
       ")            15\n",
       "company      15\n",
       ":            13\n",
       "last         13\n",
       "firm         13\n",
       "trump        13\n",
       "uk           13\n",
       "financial    13\n",
       "european     13\n",
       "eu           13\n",
       "two          12\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = edu\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"            53\n",
       "school       47\n",
       "'            45\n",
       "students     38\n",
       "s            31\n",
       "-            23\n",
       "will         22\n",
       "education    20\n",
       "trump        15\n",
       "president    12\n",
       "new          12\n",
       "children     11\n",
       "student      10\n",
       ".\"           10\n",
       "said         10\n",
       "parents      10\n",
       "law          10\n",
       "time         10\n",
       "schools      10\n",
       "first        10\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'             29\n",
       "s             24\n",
       "-             24\n",
       "school        23\n",
       "students      21\n",
       "\"             19\n",
       "education     11\n",
       "said           9\n",
       ",\"             9\n",
       "student        8\n",
       "year           8\n",
       "percent        7\n",
       "children       6\n",
       "president      6\n",
       "according      5\n",
       "will           5\n",
       "college        5\n",
       ")              5\n",
       "(              5\n",
       "university     5\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = entmt\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       ",            161\n",
       "\"            106\n",
       "s             64\n",
       "-             31\n",
       "will          29\n",
       ".\"            25\n",
       "t             24\n",
       "one           17\n",
       "time          16\n",
       "show          16\n",
       "new           16\n",
       "also          14\n",
       "said          14\n",
       "fans          13\n",
       "way           12\n",
       "now           11\n",
       "last          11\n",
       "just          11\n",
       "(             11\n",
       "character     10\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"        151\n",
       "s        102\n",
       "-         89\n",
       "said      42\n",
       ",         30\n",
       ".\"        29\n",
       "also      23\n",
       "t         21\n",
       "will      20\n",
       "one       16\n",
       "film      16\n",
       "year      16\n",
       "first     15\n",
       "told      14\n",
       "new       14\n",
       "--        14\n",
       "news      13\n",
       "show      13\n",
       "john      11\n",
       "years     11\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = polit\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "trump         79\n",
       "'             69\n",
       "\"             59\n",
       "s             56\n",
       "president     50\n",
       "clinton       29\n",
       "-             25\n",
       "donald        22\n",
       "said          20\n",
       "house         16\n",
       "white         16\n",
       "washington    15\n",
       "will          14\n",
       ".\"            13\n",
       "just          11\n",
       "cnn           11\n",
       ")             11\n",
       "(             11\n",
       "obama         11\n",
       "us            11\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"            33\n",
       "'            32\n",
       "s            28\n",
       "trump        25\n",
       "-            20\n",
       "said         18\n",
       ",\"           12\n",
       "president    10\n",
       "mr            9\n",
       "clinton       7\n",
       "campaign      6\n",
       "t             5\n",
       "will          5\n",
       "time          5\n",
       ":             5\n",
       "first         5\n",
       "u             4\n",
       "america       4\n",
       "press         4\n",
       "order         4\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = sports\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       ",         148\n",
       "\"          55\n",
       "s          51\n",
       "-          41\n",
       "will       26\n",
       "game       24\n",
       "team       23\n",
       ".\"         21\n",
       "said       18\n",
       "two        16\n",
       "one        14\n",
       "years      13\n",
       "year       13\n",
       "last       12\n",
       "time       11\n",
       "new        10\n",
       "brazil     10\n",
       "world      10\n",
       "sports      9\n",
       "just        9\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-          124\n",
       "s           83\n",
       "\"           71\n",
       "will        25\n",
       "year        23\n",
       "said        21\n",
       "world       17\n",
       "game        16\n",
       "sport       16\n",
       "one         15\n",
       ".\"          15\n",
       "two         14\n",
       "win         14\n",
       "time        13\n",
       "6           13\n",
       "team        13\n",
       "federer     12\n",
       "old         12\n",
       "sports      11\n",
       "(           11\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic = tech\n",
      "top words for fake news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'           49\n",
       "s           43\n",
       "will        42\n",
       "\"           36\n",
       "new         34\n",
       "-           34\n",
       "can         16\n",
       "amazon      14\n",
       "google      13\n",
       "now         12\n",
       "many        11\n",
       "apple       10\n",
       "t            9\n",
       "world        9\n",
       "devices      9\n",
       "said         9\n",
       "(            9\n",
       "time         9\n",
       "research     8\n",
       "app          8\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top words for real news articles\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-             29\n",
       "'             27\n",
       "s             21\n",
       "\"             15\n",
       "will          14\n",
       "said          14\n",
       "new           12\n",
       ",\"             9\n",
       "also           7\n",
       "devices        7\n",
       "can            6\n",
       "google         6\n",
       "year           6\n",
       "like           6\n",
       "t              6\n",
       "announced      5\n",
       "monday         5\n",
       "see            5\n",
       "game           4\n",
       "technology     4\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# per-topic\n",
    "article_topics = fake_news_data.loc[:, 'topic'].unique()\n",
    "en_stops = get_stop_words('en')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "top_k = 20\n",
    "for topic_i in article_topics:\n",
    "    print(f'topic = {topic_i}')\n",
    "    fake_news_data_i = fake_news_data[fake_news_data.loc[:, 'topic']==topic_i]\n",
    "    real_news_data_i = real_news_data[real_news_data.loc[:, 'topic']==topic_i]\n",
    "    # get vocab, compute counts, etc.\n",
    "    cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
    "                         tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
    "                         ngram_range=(1,1))\n",
    "    combined_txt_i = fake_news_data_i.loc[:, 'text'].append(real_news_data_i.loc[:, 'text'])\n",
    "    combined_txt_dtm_i = cv.fit_transform(combined_txt_i)\n",
    "    sorted_vocab_i = list(sorted(cv.vocabulary_.keys(), key=cv.vocabulary_.get))\n",
    "    # get separate DTM for each news data\n",
    "    cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
    "                         tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
    "                         ngram_range=(1,1), vocabulary=sorted_vocab_i)\n",
    "    fake_news_dtm_i = cv.fit_transform(fake_news_data_i.loc[:, 'text'].values)\n",
    "    real_news_dtm_i = cv.fit_transform(real_news_data_i.loc[:, 'text'].values)\n",
    "    # get top counts\n",
    "    fake_news_dtm_top_words_i = pd.Series(np.array(fake_news_dtm_i.sum(axis=0))[0], index=sorted_vocab_i).sort_values(ascending=False).head(top_k)\n",
    "    real_news_dtm_top_words_i = pd.Series(np.array(real_news_dtm_i.sum(axis=0))[0], index=sorted_vocab_i).sort_values(ascending=False).head(top_k)\n",
    "    print('top words for fake news articles')\n",
    "    display(fake_news_dtm_top_words_i)\n",
    "    print('top words for real news articles')\n",
    "    display(real_news_dtm_top_words_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_frequency(text_data, tokenizer, stops, vocab):\n",
    "    cv = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops,\n",
    "                         ngram_range=(1,1), vocabulary=vocab)\n",
    "    dtm = cv.fit_transform(text_data)\n",
    "    word_frequency = np.array(dtm.sum(axis=0))[0]\n",
    "    word_frequency = pd.Series(word_frequency, index=vocab)\n",
    "    return word_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words that occurred in more fake news articles\n",
      ",            537\n",
      "will          74\n",
      "trump         66\n",
      "president     41\n",
      "new           39\n",
      ".\"            38\n",
      "many          33\n",
      "clinton       27\n",
      "donald        25\n",
      "time          24\n",
      "now           24\n",
      "school        24\n",
      "can           23\n",
      "stated        20\n",
      "students      19\n",
      "even          18\n",
      "white         18\n",
      "order         16\n",
      "way           16\n",
      "great         16\n",
      "dtype: int64\n",
      "words that occurred in more real news articles\n",
      ")             -8\n",
      "000           -9\n",
      "â€“             -9\n",
      "6            -10\n",
      "report       -11\n",
      "m            -11\n",
      "4            -11\n",
      "tuesday      -11\n",
      "three        -11\n",
      "1            -12\n",
      "financial    -12\n",
      "$            -16\n",
      "--           -17\n",
      "also         -19\n",
      ":            -20\n",
      "year         -27\n",
      "s            -45\n",
      "said         -58\n",
      "'            -67\n",
      "-           -162\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/CORE_tutorial/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "fake_news_text = fake_news_data.loc[:, 'text'].values\n",
    "real_news_text = real_news_data.loc[:, 'text'].values\n",
    "fake_news_word_frequency = compute_frequency(fake_news_text, tokenizer, en_stops, sorted_vocab)\n",
    "real_news_word_frequency = compute_frequency(real_news_text, tokenizer, en_stops, sorted_vocab)\n",
    "# compute difference\n",
    "fake_vs_real_news_word_frequency_diff = fake_news_word_frequency - real_news_word_frequency\n",
    "fake_vs_real_news_word_frequency_diff.sort_values(inplace=True, ascending=False)\n",
    "# show words with highest/lowest difference\n",
    "top_k = 20\n",
    "print('words that occurred in more fake news articles')\n",
    "print(fake_vs_real_news_word_frequency_diff.head(top_k))\n",
    "print('words that occurred in more real news articles')\n",
    "print(fake_vs_real_news_word_frequency_diff.tail(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These differences suggest that fake news articles focused more on the actions of specific people (`trump`, `clinton`) and less on specific details (`tuesday`, `financial`).\n",
    "\n",
    "However, these results could be due to longer articles that allowed e.g. real news writers to cover more details. How do we control for length?\n",
    "\n",
    "Let's compute the normalized frequency for fake news and real news articles, to identify words that occurred more often than expected in one genre of article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_norm_frequency(text_data, tokenizer, stops, vocab):\n",
    "    cv = CountVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops,\n",
    "                         ngram_range=(1,1), vocabulary=vocab)\n",
    "    dtm = cv.fit_transform(text_data)\n",
    "    # normalize by column\n",
    "    word_norm_frequency = np.array(dtm.sum(axis=0) / dtm.sum(axis=0).sum())[0]\n",
    "    # store in format that is easy to manipulate\n",
    "    word_norm_frequency = pd.Series(word_norm_frequency, index=vocab)\n",
    "    return word_norm_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words that occurred in more fake news articles\n",
      "hillary      11.724431\n",
      "commented    10.886971\n",
      "needs        10.049512\n",
      "secret        9.212053\n",
      "caused        8.374593\n",
      "ai            8.374593\n",
      "provided      7.537134\n",
      "earth         6.699675\n",
      "begin         6.699675\n",
      "instead       6.699675\n",
      "attempt       5.862215\n",
      "release       5.862215\n",
      "success       5.862215\n",
      "stein         5.862215\n",
      "lack          5.862215\n",
      "charges       5.024756\n",
      "tennis        5.024756\n",
      "met           5.024756\n",
      "phone         5.024756\n",
      "groups        5.024756\n",
      "dtype: float64\n",
      "words that occurred in more real news articles\n",
      "anniversary    0.167492\n",
      "customer       0.167492\n",
      "missing        0.167492\n",
      "saw            0.167492\n",
      "value          0.167492\n",
      "jersey         0.167492\n",
      "providers      0.167492\n",
      "potentially    0.167492\n",
      "growing        0.139577\n",
      "indian         0.139577\n",
      "story          0.139577\n",
      "drawn          0.139577\n",
      "january        0.139577\n",
      "february       0.139577\n",
      "vehicle        0.119637\n",
      "brady          0.119637\n",
      "40             0.119637\n",
      "18             0.119637\n",
      "amateur        0.119637\n",
      "birth          0.104682\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/CORE_tutorial/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    }
   ],
   "source": [
    "tokenizer = WordPunctTokenizer()\n",
    "stops = get_stop_words('en')\n",
    "fake_news_word_norm_frequency = compute_norm_frequency(fake_news_data.loc[:, 'text'].values, tokenizer, stops, sorted_vocab)\n",
    "real_news_word_norm_frequency = compute_norm_frequency(real_news_data.loc[:, 'text'].values, tokenizer, stops, sorted_vocab)\n",
    "## compute ratio: what words are used more often in fake news than real news?\n",
    "def compute_text_word_ratio(text_data_1, text_data_2):\n",
    "    text_word_ratio = text_data_1 / text_data_2\n",
    "    # drop non-occurring words\n",
    "    text_word_ratio = text_word_ratio[~np.isinf(text_word_ratio)]\n",
    "    text_word_ratio = text_word_ratio[~np.isnan(text_word_ratio)]\n",
    "    text_word_ratio = text_word_ratio[text_word_ratio != 0.]\n",
    "    text_word_ratio.sort_values(inplace=True, ascending=False)\n",
    "    return text_word_ratio\n",
    "fake_vs_real_news_word_frequency_ratio = compute_text_word_ratio(fake_news_word_norm_frequency, real_news_word_norm_frequency)\n",
    "# show words with highest/lowest ratio\n",
    "top_k = 20\n",
    "print('words that occurred in more fake news articles')\n",
    "print(fake_real_news_word_frequency_ratio.head(top_k))\n",
    "print('words that occurred in more real news articles')\n",
    "print(fake_real_news_word_frequency_ratio.tail(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! We see that fake news consistently focuses on `hillary` (e.g. her email case) s well as potential conspiracy theories (`secret`, `ai`). In contrast, real news focuses on concrete time details (`january`, `40`) and provides some words to \"hedge\" their claims (`potentially`, `story`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to identify words that occur frequency in just a few documents? E.g. some fake news stories may disproportionately use rare but inflammatory words.\n",
    "\n",
    "Let's try TF-IDF, which normalizes term frequency by the inverse document frequency:\n",
    "\n",
    "$$\\text{tf-idf(word)} = \\frac{\\text{freq(word)}}{\\text{document-freq(word)}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def compute_non_zero_mean(data):\n",
    "    non_zero_data = data[data != 0.]\n",
    "    non_zero_mean = non_zero_data.mean()\n",
    "    return non_zero_mean\n",
    "def compute_tfidf(text_data, tokenizer, stops, vocab):\n",
    "    tfidf_vec = TfidfVectorizer(tokenizer=tokenizer.tokenize, stop_words=stops, vocabulary=vocab)\n",
    "    text_tfidf_matrix = tfidf_vec.fit_transform(text_data).toarray()\n",
    "#     print(text_tfidf_matrix.shape)\n",
    "#     return text_tfidf_matrix\n",
    "    # compute mean over non-zero TF-IDF values\n",
    "#     text_tfidf_score = np.apply_along_axis(lambda x: x.mean(), 0, text_tfidf_matrix)\n",
    "    text_tfidf_score = np.apply_along_axis(lambda x: compute_non_zero_mean(x), 0, text_tfidf_matrix)\n",
    "#     text_tfidf_score = text_tfidf_matrix.max(axis=0)\n",
    "    text_tfidf_score = pd.Series(text_tfidf_score, index=vocab)\n",
    "    return text_tfidf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with higher TF-IDF scores in fake news\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-8692082ea0c5>:4: RuntimeWarning: Mean of empty slice.\n",
      "  non_zero_mean = non_zero_data.mean()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "steel         4.280091\n",
       "retailers     4.089067\n",
       "tourists      3.949276\n",
       "friendship    3.927483\n",
       "morgan        3.752379\n",
       "bruno         3.728585\n",
       "gas           3.509625\n",
       "saudi         3.392921\n",
       "arnold        3.364493\n",
       "privacy       3.356762\n",
       "sacrifice     3.214699\n",
       "emoji         3.112207\n",
       "duncan        3.080392\n",
       "michelle      3.053198\n",
       "ebony         3.014936\n",
       "qatar         2.980143\n",
       "fees          2.926611\n",
       "ai            2.926181\n",
       "wawrinka      2.924895\n",
       "kyrgios       2.892984\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with higher TF-IDF scores in real news\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "virtual           0.331943\n",
       "comfortable       0.330759\n",
       "saran             0.312199\n",
       "hacking           0.309510\n",
       "junco             0.308395\n",
       "farah             0.303634\n",
       "iphones           0.300928\n",
       "putin             0.297846\n",
       "engines           0.292603\n",
       "fisher            0.280006\n",
       "alcohol           0.279075\n",
       "absurdity         0.277041\n",
       "suddenly          0.272967\n",
       "pizzagate         0.270884\n",
       "punk              0.265890\n",
       "authentication    0.264018\n",
       "factor            0.264018\n",
       "graduates         0.254588\n",
       "tempe             0.239919\n",
       "investigators     0.221240\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fake_news_tfidf = compute_tfidf(fake_news_text, tokenizer, en_stops, sorted_vocab)\n",
    "real_news_tfidf = compute_tfidf(real_news_text, tokenizer, en_stops, sorted_vocab)\n",
    "fake_vs_real_news_word_tfidf_ratio = fake_news_tfidf / real_news_tfidf\n",
    "fake_vs_real_news_word_tfidf_ratio.dropna(inplace=True)\n",
    "fake_vs_real_news_word_tfidf_ratio.sort_values(inplace=True, ascending=False)\n",
    "top_k = 20\n",
    "print('words with higher TF-IDF scores in fake news')\n",
    "display(fake_vs_real_news_word_tfidf_ratio.head(top_k))\n",
    "print('words with higher TF-IDF scores in real news')\n",
    "display(fake_vs_real_news_word_tfidf_ratio.tail(top_k))\n",
    "# raw TF-IDF scores\n",
    "# fake_news_tfidf.sort_values(inplace=True, ascending=False)\n",
    "# real_news_tfidf.sort_values(inplace=True, ascending=False)\n",
    "# top_k = 20\n",
    "# print('words with high TF-IDF scores in fake news')\n",
    "# print(fake_news_tfidf.head(top_k))\n",
    "# print('words with high TF-IDF scores in real news')\n",
    "# print(real_news_tfidf.head(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method succeeds in identifying fairly rare words that characterize real and fake news.\n",
    "\n",
    "For fake news, we see that words with higher TF-IDF scores include those related to business transactions (`retailers`, `gas`) and Middle Eastern countries (`saudi`, `qatar`).\n",
    "\n",
    "For real news, the words with higher TF-IDF scores include words that directly address conspiracies (`pizzagate`, `investigators`, `authentication`) and words that speculate on the veracity of claims (`absurdity`, `suddenly`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "\n",
    "Now it's time for you to explore the data a little more with word frequency modeling!\n",
    "\n",
    "Some thoughts:\n",
    "\n",
    "- The original data are organized by topic. What are the words that characterize real/fake news in each topic?\n",
    "- Changing the vocabulary size could identify more rare words (e.g. lowering `min_df` threshold in `CountVectorizer`). What happens if you include more words in the vocabulary?\n",
    "- Up until now we have focused more strongly on single words (unigrams). What if we include phrases (changing `ngram_range` in the `CountVectorizer`)? Will we see more examples of conspiracy theories being highlighted by the real news?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top unigrams/bigrams that occur more often in fake news data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/CORE_tutorial/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "hillary clinton     10.837533\n",
       ", \"                  7.711322\n",
       "however ,            7.502908\n",
       "trump .              7.502908\n",
       ", many               6.669251\n",
       ", one                5.835595\n",
       "game ,               5.001938\n",
       "president donald     5.001938\n",
       "couldn '             5.001938\n",
       "first lady           5.001938\n",
       "white house          4.724053\n",
       "donald trump         4.335013\n",
       "monday .             4.168282\n",
       "night ,              4.168282\n",
       "now ,                4.168282\n",
       "trump tower          4.168282\n",
       "\" just               4.168282\n",
       ". new                3.751454\n",
       ", wanted             3.334626\n",
       "supreme court        3.334626\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top unigrams/bigrams that occur more often in real news data\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "economy .        0.208414\n",
       "performance -    0.208414\n",
       "2 .              0.208414\n",
       "well -           0.208414\n",
       ". k              0.208414\n",
       ". 4              0.208414\n",
       "2016 .           0.208414\n",
       ". still          0.208414\n",
       "s really         0.208414\n",
       ", adding         0.208414\n",
       "science ,        0.208414\n",
       "k .              0.208414\n",
       "- year           0.189467\n",
       "year -           0.185257\n",
       "- old            0.175507\n",
       "middle east      0.166731\n",
       "indian wells     0.166731\n",
       ". report         0.166731\n",
       "world number     0.166731\n",
       "1 .              0.083366\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## example: test different n-gram range\n",
    "## generate new vocabulary\n",
    "tokenizer = WordPunctTokenizer()\n",
    "en_stops = get_stop_words('en')\n",
    "def compute_word_freq_custom(text_data, custom_cv, vocab):\n",
    "    text_dtm = custom_cv.transform(text_data)\n",
    "    word_norm_frequency = np.array(text_dtm.sum(axis=0) / text_dtm.sum(axis=0).sum())[0]\n",
    "    word_norm_frequency = pd.Series(word_norm_frequency, index=vocab)\n",
    "    return word_norm_frequency\n",
    "# create custom vectorizer for bigrams\n",
    "bigram_cv = CountVectorizer(min_df=0.001, max_df=0.75, \n",
    "                            tokenizer=tokenizer.tokenize, stop_words=en_stops,\n",
    "                            ngram_range=(2,2))\n",
    "# get vocab for all data\n",
    "combined_txt = fake_news_data.loc[:, 'text'].append(real_news_data.loc[:, 'text'])\n",
    "combined_txt_dtm = bigram_cv.fit_transform(combined_txt)\n",
    "sorted_bigram_vocab = list(sorted(bigram_cv.vocabulary_.keys(), key=bigram_cv.vocabulary_.get))\n",
    "## compute frequency ratio for bigrams\n",
    "fake_news_bigram_frequency = compute_word_freq_custom(fake_news_data.loc[:, 'text'].values, bigram_cv, sorted_bigram_vocab)\n",
    "fake_vs_real_news_bigram_word_frequency_ratio = compute_text_word_ratio(fake_news_bigram_frequency, real_news_bigram_frequency)\n",
    "fake_vs_real_news_bigram_word_frequency_ratio.sort_values(inplace=True, ascending=False)\n",
    "top_k = 20\n",
    "print('top unigrams/bigrams that occur more often in fake news data')\n",
    "display(fake_vs_real_news_bigram_word_frequency_ratio.head(top_k))\n",
    "print('top unigrams/bigrams that occur more often in real news data')\n",
    "display(fake_vs_real_news_bigram_word_frequency_ratio.tail(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Topic modeling\n",
    "Another way to compare documents is to extract the latent topics that group words within each document, and compare those distributions.\n",
    "\n",
    "We'll continue on the topic of fake news with another dataset that has examples of both fake and real news articles, at a much larger scale than the previous data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Donald Trump just couldn t wish all Americans a Happy New Year and leave it at that. Instead, he had to give a shout out to his enemies, haters and  the very dishonest fake news media.  The former reality show star had just one job to do and he couldn t do it. As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year,  President Angry Pants tweeted.  2018 will be a great year for America! As our Country rapidly grows stronger and smarter, I want to wish all of my friends, supporters, enemies, haters, and even the very dishonest Fake News Media, a Happy and Healthy New Year. 2018 will be a great year for America!  Donald J. Trump (@realDonaldTrump) December 31, 2017Trump s tweet went down about as welll as you d expect.What kind of president sends a New Year s greeting like this despicable, petty, infantile gibberish? Only Trump! His lack of decency won t even allow him to rise above the gutter long enough to wish the American citizens a happy new year!  Bishop Talbert Swan (@TalbertSwan) December 31, 2017no one likes you  Calvin (@calvinstowell) December 31, 2017Your impeachment would make 2018 a great year for America, but I ll also accept regaining control of Congress.  Miranda Yaver (@mirandayaver) December 31, 2017Do you hear yourself talk? When you have to include that many people that hate you you have to wonder? Why do the they all hate me?  Alan Sandoval (@AlanSandoval13) December 31, 2017Who uses the word Haters in a New Years wish??  Marlene (@marlene399) December 31, 2017You can t just say happy new year?  Koren pollitt (@Korencarpenter) December 31, 2017Here s Trump s New Year s Eve tweet from 2016.Happy New Year to all, including to my many enemies and those who have fought me and lost so badly they just don t know what to do. Love!  Donald J. Trump (@realDonaldTrump) December 31, 2016This is nothing new for Trump. He s been doing this for years.Trump has directed messages to his  enemies  and  haters  for New Year s, Easter, Thanksgiving, and the anniversary of 9/11. pic.twitter.com/4FPAe2KypA  Daniel Dale (@ddale8) December 31, 2017Trump s holiday tweets are clearly not presidential.How long did he work at Hallmark before becoming President?  Steven Goodine (@SGoodine) December 31, 2017He s always been like this . . . the only difference is that in the last few years, his filter has been breaking down.  Roy Schulze (@thbthttt) December 31, 2017Who, apart from a teenager uses the term haters?  Wendy (@WendyWhistles) December 31, 2017he s a fucking 5 year old  Who Knows (@rainyday80) December 31, 2017So, to all the people who voted for this a hole thinking he would change once he got into power, you were wrong! 70-year-old men don t change and now he s a year older.Photo by Andrew Burton/Getty Images.',\n",
       "       'House Intelligence Committee Chairman Devin Nunes is going to have a bad day. He s been under the assumption, like many of us, that the Christopher Steele-dossier was what prompted the Russia investigation so he s been lashing out at the Department of Justice and the FBI in order to protect Trump. As it happens, the dossier is not what started the investigation, according to documents obtained by the New York Times.Former Trump campaign adviser George Papadopoulos was drunk in a wine bar when he revealed knowledge of Russian opposition research on Hillary Clinton.On top of that, Papadopoulos wasn t just a covfefe boy for Trump, as his administration has alleged. He had a much larger role, but none so damning as being a drunken fool in a wine bar. Coffee boys  don t help to arrange a New York meeting between Trump and President Abdel Fattah el-Sisi of Egypt two months before the election. It was known before that the former aide set up meetings with world leaders for Trump, but team Trump ran with him being merely a coffee boy.In May 2016, Papadopoulos revealed to Australian diplomat Alexander Downer that Russian officials were shopping around possible dirt on then-Democratic presidential nominee Hillary Clinton. Exactly how much Mr. Papadopoulos said that night at the Kensington Wine Rooms with the Australian, Alexander Downer, is unclear,  the report states.  But two months later, when leaked Democratic emails began appearing online, Australian officials passed the information about Mr. Papadopoulos to their American counterparts, according to four current and former American and foreign officials with direct knowledge of the Australians  role. Papadopoulos pleaded guilty to lying to the F.B.I. and is now a cooperating witness with Special Counsel Robert Mueller s team.This isn t a presidency. It s a badly scripted reality TV show.Photo by Win McNamee/Getty Images.',\n",
       "       'On Friday, it was revealed that former Milwaukee Sheriff David Clarke, who was being considered for Homeland Security Secretary in Donald Trump s administration, has an email scandal of his own.In January, there was a brief run-in on a plane between Clarke and fellow passenger Dan Black, who he later had detained by the police for no reason whatsoever, except that maybe his feelings were hurt. Clarke messaged the police to stop Black after he deplaned, and now, a search warrant has been executed by the FBI to see the exchanges.Clarke is calling it fake news even though copies of the search warrant are on the Internet. I am UNINTIMIDATED by lib media attempts to smear and discredit me with their FAKE NEWS reports designed to silence me,  the former sheriff tweeted.  I will continue to poke them in the eye with a sharp stick and bitch slap these scum bags til they get it. I have been attacked by better people than them #MAGA I am UNINTIMIDATED by lib media attempts to smear and discredit me with their FAKE NEWS reports designed to silence me. I will continue to poke them in the eye with a sharp stick and bitch slap these scum bags til they get it. I have been attacked by better people than them #MAGA pic.twitter.com/XtZW5PdU2b  David A. Clarke, Jr. (@SheriffClarke) December 30, 2017He didn t stop there.BREAKING NEWS! When LYING LIB MEDIA makes up FAKE NEWS to smear me, the ANTIDOTE is go right at them. Punch them in the nose & MAKE THEM TASTE THEIR OWN BLOOD. Nothing gets a bully like LYING LIB MEDIA S attention better than to give them a taste of their own blood #neverbackdown pic.twitter.com/T2NY2psHCR  David A. Clarke, Jr. (@SheriffClarke) December 30, 2017The internet called him out.This is your local newspaper and that search warrant isn t fake, and just because the chose not to file charges at the time doesn t mean they won t! Especially if you continue to lie. Months after decision not to charge Clarke, email search warrant filed https://t.co/zcbyc4Wp5b  KeithLeBlanc (@KeithLeBlanc63) December 30, 2017I just hope the rest of the Village People aren t implicated.  Kirk Ketchum (@kirkketchum) December 30, 2017Slaw, baked potatoes, or French fries? pic.twitter.com/fWfXsZupxy  ALT- Immigration   (@ALT_uscis) December 30, 2017pic.twitter.com/ymsOBLjfxU  Pendulum Swinger (@PendulumSwngr) December 30, 2017you called your police friends to stand up for you when someone made fun of your hat  Chris Jackson (@ChrisCJackson) December 30, 2017Is it me, with this masterful pshop of your hat, which I seem to never tire of. I think it s the steely resolve in your one visible eye pic.twitter.com/dWr5k8ZEZV  Chris Mohney (@chrismohney) December 30, 2017Are you indicating with your fingers how many people died in your jail? I think you re a few fingers short, dipshit  Ike Barinholtz (@ikebarinholtz) December 30, 2017ROFL. Internet tough guy with fake flair. pic.twitter.com/ulCFddhkdy  KellMeCrazy (@Kel_MoonFace) December 30, 2017You re so edgy, buddy.  Mrs. SMH (@MRSSMH2) December 30, 2017Is his break over at Applebees?  Aaron (@feltrrr2) December 30, 2017Are you trying to earn your  still relevant  badge?  CircusRebel (@CircusDrew) December 30, 2017make sure to hydrate, drink lots of water. It s rumored that prisoners can be denied water by prison officials.  Robert Klinc (@RobertKlinc1) December 30, 2017Terrill Thomas, the 38-year-old black man who died of thirst in Clarke s Milwaukee County Jail cell this April, was a victim of homicide. We just thought we should point that out. It can t be repeated enough.Photo by Spencer Platt/Getty Images.',\n",
       "       'On Christmas day, Donald Trump announced that he would  be back to work  the following day, but he is golfing for the fourth day in a row. The former reality show star blasted former President Barack Obama for playing golf and now Trump is on track to outpace the number of golf games his predecessor played.Updated my tracker of Trump s appearances at Trump properties.71 rounds of golf including today s. At this pace, he ll pass Obama s first-term total by July 24 next year. https://t.co/Fg7VacxRtJ pic.twitter.com/5gEMcjQTbH  Philip Bump (@pbump) December 29, 2017 That makes what a Washington Post reporter discovered on Trump s website really weird, but everything about this administration is bizarre AF. The coding contained a reference to Obama and golf:  Unlike Obama, we are working to fix the problem   and not on the golf course.  However, the coding wasn t done correctly.The website of Donald Trump, who has spent several days in a row at the golf course, is coded to serve up the following message in the event of an internal server error: https://t.co/zrWpyMXRcz pic.twitter.com/wiQSQNNzw0  Christopher Ingraham (@_cingraham) December 28, 2017That snippet of code appears to be on all https://t.co/dkhw0AlHB4 pages, which the footer says is paid for by the RNC? pic.twitter.com/oaZDT126B3  Christopher Ingraham (@_cingraham) December 28, 2017It s also all over https://t.co/ayBlGmk65Z. As others have noted in this thread, this is weird code and it s not clear it would ever actually display, but who knows.  Christopher Ingraham (@_cingraham) December 28, 2017After the coding was called out, the reference to Obama was deleted.UPDATE: The golf error message has been removed from the Trump and GOP websites. They also fixed the javascript  =  vs  ==  problem. Still not clear when these messages would actually display, since the actual 404 (and presumably 500) page displays a different message pic.twitter.com/Z7dmyQ5smy  Christopher Ingraham (@_cingraham) December 29, 2017That suggests someone at either RNC or the Trump admin is sensitive enough to Trump s golf problem to make this issue go away quickly once people noticed. You have no idea how much I d love to see the email exchange that led us here.  Christopher Ingraham (@_cingraham) December 29, 2017 The code was f-cked up.The best part about this is that they are using the  =  (assignment) operator which means that bit of code will never get run. If you look a few lines up  errorCode  will always be  404          (@tw1trsux) December 28, 2017trump s coders can t code. Nobody is surprised.  Tim Peterson (@timrpeterson) December 28, 2017Donald Trump is obsessed with Obama that his name was even in the coding of his website while he played golf again.Photo by Joe Raedle/Getty Images.',\n",
       "       'Pope Francis used his annual Christmas Day message to rebuke Donald Trump without even mentioning his name. The Pope delivered his message just days after members of the United Nations condemned Trump s move to recognize Jerusalem as the capital of Israel. The Pontiff prayed on Monday for the  peaceful coexistence of two states within mutually agreed and internationally recognized borders. We see Jesus in the children of the Middle East who continue to suffer because of growing tensions between Israelis and Palestinians,  Francis said.  On this festive day, let us ask the Lord for peace for Jerusalem and for all the Holy Land. Let us pray that the will to resume dialogue may prevail between the parties and that a negotiated solution can finally be reached. The Pope went on to plead for acceptance of refugees who have been forced from their homes, and that is an issue Trump continues to fight against. Francis used Jesus for which there was  no place in the inn  as an analogy. Today, as the winds of war are blowing in our world and an outdated model of development continues to produce human, societal and environmental decline, Christmas invites us to focus on the sign of the Child and to recognize him in the faces of little children, especially those for whom, like Jesus,  there is no place in the inn,  he said. Jesus knows well the pain of not being welcomed and how hard it is not to have a place to lay one s head,  he added.  May our hearts not be closed as they were in the homes of Bethlehem. The Pope said that Mary and Joseph were immigrants who struggled to find a safe place to stay in Bethlehem. They had to leave their people, their home, and their land,  Francis said.  This was no comfortable or easy journey for a young couple about to have a child.   At heart, they were full of hope and expectation because of the child about to be born; yet their steps were weighed down by the uncertainties and dangers that attend those who have to leave their home behind. So many other footsteps are hidden in the footsteps of Joseph and Mary,  Francis said Sunday. We see the tracks of entire families forced to set out in our own day. We see the tracks of millions of persons who do not choose to go away, but driven from their land, leave behind their dear ones. Amen to that.Photo by Christopher Furlong/Getty Images.',\n",
       "       'The number of cases of cops brutalizing and killing people of color seems to see no end. Now, we have another case that needs to be shared far and wide. An Alabama woman by the name of Angela Williams shared a graphic photo of her son, lying in a hospital bed with a beaten and fractured face, on Facebook. It needs to be shared far and wide, because this is unacceptable.It is unclear why Williams  son was in police custody or what sort of altercation resulted in his arrest, but when you see the photo you will realize that these details matter not. Cops are not supposed to beat and brutalize those in their custody. In the post you are about to see, Ms. Williams expresses her hope that the cops had their body cameras on while they were beating her son, but I think we all know that there will be some kind of convenient  malfunction  to explain away the lack of existence of dash or body camera footage of what was clearly a brutal beating. Hell, it could even be described as attempted murder. Something tells me that this young man will never be the same. Without further ado, here is what Troy, Alabama s finest decided was appropriate treatment of Angela Williams  son:No matter what the perceived crime of this young man might be, this is completely unacceptable. The cops who did this need to rot in jail for a long, long time   but what you wanna bet they get a paid vacation while the force  investigates  itself, only to have the officers returned to duty posthaste?This, folks, is why we say BLACK LIVES MATTER. No way in hell would this have happened if Angela Williams  son had been white. Please share far and wide, and stay tuned to Addicting Info for further updates.Featured image via David McNew/Stringer/Getty Images',\n",
       "       'Donald Trump spent a good portion of his day at his golf club, marking the 84th day he s done so since taking the oath of office. It must have been a bad game because just after that, Trump lashed out at FBI Deputy Director Andrew McCabe on Twitter following a report saying McCabe plans to retire in a few months. The report follows McCabe s testimony in front of congressional committees this week, as well as mounting criticism from Republicans regarding the Russia probe.So, naturally, Trump attacked McCabe with a lie. How can FBI Deputy Director Andrew McCabe, the man in charge, along with leakin  James Comey, of the Phony Hillary Clinton investigation (including her 33,000 illegally deleted emails) be given $700,000 for wife s campaign by Clinton Puppets during investigation?  Trump tweeted.How can FBI Deputy Director Andrew McCabe, the man in charge, along with leakin  James Comey, of the Phony Hillary Clinton investigation (including her 33,000 illegally deleted emails) be given $700,000 for wife s campaign by Clinton Puppets during investigation?  Donald J. Trump (@realDonaldTrump) December 23, 2017He didn t stop there.FBI Deputy Director Andrew McCabe is racing the clock to retire with full benefits. 90 days to go?!!!  Donald J. Trump (@realDonaldTrump) December 23, 2017Wow,  FBI lawyer James Baker reassigned,  according to @FoxNews.  Donald J. Trump (@realDonaldTrump) December 23, 2017With all of the Intel at Trump s disposal, he s getting his information from Fox News. McCabe spent most of his career in the fight against terrorism and now he s being attacked by the so-called president. Trump has been fact-checked before on his claim of his wife receiving $700,000 for her campaign.Politifact noted in late July that Trump s  tweet about Andrew McCabe is a significant distortion of the facts. And the implication that McCabe got Clinton off as a political favor doesn t make much sense when we look at the evidence. His July tweet was rated  mostly false.  But Trump repeats these lies because he knows his supporters will believe them without bothering to Google. It s still a lie, though.Photo by Zach Gibson   Pool/Getty Images.',\n",
       "       'In the wake of yet another court decision that derailed Donald Trump s plan to bar Muslims from entering the United States, the New York Times published a report on Saturday morning detailing the president s frustration at not getting his way   and how far back that frustration goes.According to the article, back in June, Trump stomped into the Oval Office, furious about the state of the travel ban, which he thought would be implemented and fully in place by then. Instead, he fumed, visas had already been issued to immigrants at such a rate that his  friends were calling to say he looked like a fool  after making his broad pronouncements.It was then that Trump began reading from a document that a top advisor, noted white supremacist Stephen Miller, had handed him just before the meeting with his Cabinet. The page listed how many visas had been issued this year, and included 2,500 from Afghanistan (a country not on the travel ban), 15,000 from Haiti (also not included), and 40,000 from Nigeria (sensing a pattern yet?), and Trump expressed his dismay at each.According to witnesses in the room who spoke to the Times on condition of anonymity, and who were interviewed along with three dozen others for the article, Trump called out each country for its faults as he read: Afghanistan was a  terrorist haven,  the people of Nigeria would  never go back to their huts once they saw the glory of America, and immigrants from Haiti  all have AIDS. Despite the extensive research done by the newspaper, the White House of course denies that any such language was used.But given Trump s racist history and his advisor Stephen Miller s blatant white nationalism, it would be no surprise if a Freedom of Information Act request turned up that the document in question had the statements printed inline as commentary for the president to punctuate his anger with. It was Miller, after all, who was responsible for the  American Carnage  speech that Trump delivered at his inauguration.This racist is a menace to America, and he doesn t represent anything that this country stands for. Let s hope that more indictments from Robert Mueller are on their way as we speak.Featured image via Chris Kleponis/Pool/Getty Images',\n",
       "       'Many people have raised the alarm regarding the fact that Donald Trump is dangerously close to becoming an autocrat. The thing is, democracies become autocracies right under the people s noses, because they can often look like democracies in the beginning phases. This was explained by Republican David Frum just a couple of months into Donald Trump s presidency, in a piece in The Atlantic called  How to Build an Autocracy. In fact, if you really look critically at what is happening right now   the systematic discrediting of vital institutions such as the free press and the Federal Bureau of Investigation as well the direct weaponization of the Department of Justice in order to go after Trump s former political opponent, 2016 Democratic nominee Hillary Clinton, and you have the makings of an autocracy. We are more than well on our way. Further, one chamber of Congress, the House of Representatives, already has a rogue band of Republicans who are running a parallel investigation to the official Russian collusion investigation, with the explicit intent of undermining and discrediting the idea that Trump could have possibly done anything wrong with the Russians in order to swing the 2016 election in his favor.All of that is just for starters, too. Now, we have Trump making United Nations Ambassador Nikki Haley bully and threaten other countries in the United Nations who voted against Trump s decision to change U.S. policy when it comes to recognition of Jerusalem as the capital of the Jewish State. Well, one expert, who is usually quite measured, has had enough of Trump s autocratic antics: Former CIA Director John O. Brennan. The seasoned spy took to Trump s favorite platform, Twitter, and blasted the decision:Trump Admin threat to retaliate against nations that exercise sovereign right in UN to oppose US position on Jerusalem is beyond outrageous. Shows @realDonaldTrump expects blind loyalty and subservience from everyone qualities usually found in narcissistic, vengeful autocrats.  John O. Brennan (@JohnBrennan) December 21, 2017Director Brennan is correct, of course. Trump is behaving just like an autocrat, and so many people in the nation are asleep when it comes to this dangerous age, in which the greatest threat to democracy and the very fabric of the republic itself is the American president. Fellow Americans, we know the GOP-led Congress will not be the check on Trump that they are supposed to be. It s time to get out and flip the House and possibly the Senate in 2018, and resist in the meantime, if we want to save our country from devolving into something that looks more like Russia or North Korea than the America we have always know. We re already well on our way.Featured image via BRENDAN SMIALOWSKI/AFP/Getty Images',\n",
       "       'Just when you might have thought we d get a break from watching people kiss Donald Trump s ass and stroke his ego ad nauseam, a pro-Trump group creates an ad that s nothing but people doing even more of those exact things. America First Policies is set to release this ad, called  Thank You, President Trump,  on Christmas Day and, well, we threw up a little in our mouths trying to watch this.Basically, the spot is nothing but people fawning all over Trump for all the stuff he hasn t actually done. The ad includes a scene with a little girl thanking Trump for bringing back  Merry Christmas,  which never went away (there are even videos of President Obama saying  Merry Christmas  himself). A man thanks him for cutting his taxes. And America First says that everyday Americans everywhere are thanking Trump for being such a great and awesome president.The best president.Nobody s ever done what he s done. He s breaking all kinds of records every day.Believe us.Anyway, the word  propaganda  comes to mind when watching this. That s what it is   literal propaganda promoting someone who shouldn t need this kind of promotion anymore. Watch this ad bullshit below:The way the MAGAs are kowtowing to Orange Hitler is both disgusting and frightening. The man has done nothing, and his policies will harm the very same Americans who are thanking him. Unfortunately, it will take an obscene amount of pain before they ll open their eyes and see they ve been duped by a con man with a bad hairdo.And his ongoing need for this kind of adoration is, at best, unbecoming of his office. This ad is vile.Featured image via Al Drago-Pool/Getty Images'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## data = fake news challenge\n",
    "import pandas as pd\n",
    "fake_news_article_data = pd.read_csv('data/fake_news_challenge/Fake.csv', sep=',', index_col=False)\n",
    "real_news_article_data = pd.read_csv('data/fake_news_challenge/True.csv', sep=',', index_col=False)\n",
    "display(fake_news_article_data.loc[:, 'text'].head(10).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we try topic modeling, we have to convert the text to a usable format (document-term matrix, like before)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ianbstew/miniconda3/envs/CORE_tutorial/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'\", 'aren', 'can', 'couldn', 'd', 'didn', 'doesn', 'don', 'hadn', 'hasn', 'haven', 'isn', 'let', 'll', 'm', 'mustn', 're', 's', 'shan', 'shouldn', 't', 've', 'wasn', 'weren', 'won', 'wouldn'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 13751)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from stop_words import get_stop_words\n",
    "## combine text data, keep track of fake/real news indices\n",
    "combined_news_text = fake_news_article_data.loc[:, 'text'].append(real_news_article_data.loc[:, 'text'])\n",
    "fake_news_text_indices = list(range(fake_news_article_data.shape[0]))\n",
    "real_news_text_indices = list(range(fake_news_article_data.shape[0], combined_news_text.shape[0]))\n",
    "## convert text to DTM\n",
    "en_stops = get_stop_words('en')\n",
    "tokenizer = WordPunctTokenizer()\n",
    "cv = CountVectorizer(min_df=0.001, max_df=0.75, lowercase=True, \n",
    "                     ngram_range=(1,1), stop_words=en_stops, tokenizer=tokenizer.tokenize)\n",
    "combined_news_text_dtm = cv.fit_transform(combined_news_text)\n",
    "print(combined_news_text_dtm.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our first method, let's try Latent Semantic Analysis, which is a form of dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 10)\n"
     ]
    }
   ],
   "source": [
    "## LSA\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "num_topics = 10\n",
    "num_iter = 10\n",
    "lsa_model = TruncatedSVD(n_components=num_topics, n_iter=num_iter, random_state=123)\n",
    "combined_news_text_lsa_topics = lsa_model.fit_transform(combined_news_text_dtm)\n",
    "print(combined_news_text_lsa_topics.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LSA process outputs continuous values [-inf, +inf] which we need to convert to probabilities [0,1]. We can use the softmax function along each dimension to convert the matrix to probabilities:\n",
    "\n",
    "$$\\text{softmax}(x_{i}) = \\frac{e^{x_{i}}}{\\sum_{j}^{K}e^{x_{j}}}$$\n",
    "\n",
    "where $x$ is one of $K$ topic dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import softmax\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "# convert per-column scores to a normal distribution (0,1)\n",
    "scaler = StandardScaler()\n",
    "combined_news_text_lsa_topic_scores = scaler.fit_transform(combined_news_text_lsa_topics)\n",
    "# soft-max per-column\n",
    "combined_news_text_lsa_topic_probs = softmax(combined_news_text_lsa_topic_scores.T).T\n",
    "# normalize per-row so that probabilities sum to 1\n",
    "combined_news_text_lsa_topic_probs = combined_news_text_lsa_topic_probs / combined_news_text_lsa_topic_probs.sum(axis=1).reshape(-1,1)\n",
    "# print(combined_news_text_lsa_topic_probs)\n",
    "# print(combined_news_text_lsa_topic_probs)\n",
    "# print(combined_news_text_lsa_topic_probs.mean(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the expected probability of a document being assigned to a topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expected probability of topics = \n",
      "0    0.415698\n",
      "1    0.000006\n",
      "2    0.000006\n",
      "3    0.000006\n",
      "4    0.000042\n",
      "5    0.302869\n",
      "6    0.000013\n",
      "7    0.280472\n",
      "8    0.000836\n",
      "9    0.000053\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "combined_news_text_lsa_expected_topics = pd.Series(combined_news_text_lsa_topic_probs.mean(axis=0))\n",
    "print(f'expected probability of topics = \\n{combined_news_text_lsa_expected_topics}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the data is \"dominated\" by 3 topics with high probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out what \"topics\" the model learned, let's look at the news articles with the highest probability for each topic.\n",
    "\n",
    "We'll take the arg-max along each topic and print the text for the corresponding articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_ids = list(range(num_topics))\n",
    "top_articles_per_topic = 5\n",
    "for topic_id in topic_ids:\n",
    "    combined_news_text_lsa_topic_probs[:, topic_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.38451383,  6.81931655, -8.99509218,  4.23043665, -4.51714638])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "combined_news_text_lsa_topics[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## testing topic parameters => looking at top words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## computing document topic proportions; compare proportions across real/fake? look at example documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stretch goal: visualizing topics??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: clean data, re-save, generate topics, compare distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Yearâ€™...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obamaâ€™s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Yearâ€™...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obamaâ€™s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  \n",
       "0  December 31, 2017  \n",
       "1  December 31, 2017  \n",
       "2  December 30, 2017  \n",
       "3  December 29, 2017  \n",
       "4  December 25, 2017  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## data = fake news challenge\n",
    "import pandas as pd\n",
    "fake_news_article_data = pd.read_csv('data/fake_news_challenge/Fake.csv', sep=',', index_col=False)\n",
    "real_news_article_data = pd.read_csv('data/fake_news_challenge/True.csv', sep=',', index_col=False)\n",
    "display(fake_news_article_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## clean data\n",
    "from nltk.tokenize import PunktSentenceTokenizer, WordPunctTokenizer\n",
    "sent_tokenizer = PunktSentenceTokenizer()\n",
    "word_tokenizer = WordPunctTokenizer()\n",
    "def get_sentence_word_tokens(text, word_tokenizer, sent_tokenizer):\n",
    "    text_sents = sent_tokenizer.tokenize(text)\n",
    "    text_sent_tokens = list(map(word_tokenizer.tokenize, text_sents))\n",
    "    return text_sent_tokens\n",
    "fake_news_sentences = fake_news_article_data.loc[:, 'text'].apply(lambda x: get_sentence_word_tokens(x, word_tokenizer, sent_tokenizer))\n",
    "real_news_sentences = real_news_article_data.loc[:, 'text'].apply(lambda x: get_sentence_word_tokens(x, word_tokenizer, sent_tokenizer))\n",
    "# flatten for processing\n",
    "from functools import reduce\n",
    "def flatten_list_data(data):\n",
    "    flat_data = []\n",
    "    for x in data:\n",
    "        flat_data.extend(x)\n",
    "    return flat_data\n",
    "fake_news_sentences = flatten_list_data(fake_news_sentences)\n",
    "real_news_sentences = flatten_list_data(real_news_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## train word2vec embeddings\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "def train_word2vec_model(text_sents, model_out_file):\n",
    "    dim = 50\n",
    "    alpha = 0.025\n",
    "    window = 5\n",
    "    min_count = 5\n",
    "    model = Word2Vec(sentences=text_sents, size=dim, alpha=alpha, window=window, min_count=min_count)\n",
    "#     model.build_vocab(text_sents)\n",
    "    model.save(model_out_file)\n",
    "fake_news_word2vec_model_out_file = 'data/fake_news_challenge/fake_news_word2vec_embed.model'\n",
    "real_news_word2vec_model_out_file = 'data/fake_news_challenge/real_news_word2vec_embed.model'\n",
    "train_word2vec_model(fake_news_sentences, fake_news_word2vec_model_out_file)\n",
    "train_word2vec_model(real_news_sentences, real_news_word2vec_model_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load from file\n",
    "fake_news_word2vec_embed_model = Word2Vec.load(fake_news_word2vec_model_out_file)\n",
    "real_news_word2vec_embed_model = Word2Vec.load(real_news_word2vec_model_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train Glove embeddings\n",
    "from glove import Glove, Corpus\n",
    "def fit_glove_model(text_sents, model_out_file):\n",
    "    dim = 50\n",
    "    learning_rate = 0.05\n",
    "    alpha = 0.025\n",
    "    random_state = 123\n",
    "    train_epochs = 100\n",
    "    num_threads = 4\n",
    "    window = 5\n",
    "    glove_corpus = Corpus()\n",
    "    glove_corpus.fit(text_sents, window=window)\n",
    "    glove_embed_model = Glove(no_components=dim, learning_rate=learning_rate, \n",
    "                              alpha=alpha, random_state=random_state)\n",
    "    # note: this takes ~ 5 minutes with 4 threads on a server\n",
    "    glove_embed_model.fit(glove_corpus.matrix, epochs=train_epochs,\n",
    "                          no_threads=num_threads, verbose=True)\n",
    "    glove_embed_model.add_dictionary(glove_corpus.dictionary)\n",
    "    glove_embed_model.save(model_out_file)\n",
    "fake_news_glove_model_out_file = 'data/fake_news_challenge/fake_news_glove_embed.model'\n",
    "real_news_glove_model_out_file = 'data/fake_news_challenge/real_news_glove_embed.model'\n",
    "print('fitting Glove embeddings for fake news')\n",
    "fit_glove_model(fake_news_sentences, fake_news_glove_model_out_file)\n",
    "print('fitting Glove embeddings for real news')\n",
    "fit_glove_model(real_news_sentences, real_news_glove_model_out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reload models after training\n",
    "fake_news_glove_embed_model = Glove.load(fake_news_glove_model_out_file)\n",
    "real_news_glove_embed_model = Glove.load(real_news_glove_model_out_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start out by looking at the nearest neighbors for some test words. \n",
    "\n",
    "We'll get the test words by filtering from the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       ".                 899948\n",
       ",                 872906\n",
       "s                 231123\n",
       "-                 191679\n",
       "Trump             132996\n",
       "said              132673\n",
       "The               115553\n",
       "â€™                  70098\n",
       ":                  63415\n",
       ")                  63128\n",
       "I                  62061\n",
       "(                  57607\n",
       "U                  54124\n",
       "â€œ                  53931\n",
       "t                  53069\n",
       "S                  51453\n",
       "will               48561\n",
       "people             39640\n",
       "President          36123\n",
       "one                32627\n",
       "also               30519\n",
       "It                 29705\n",
       "Reuters            29343\n",
       "Clinton            28556\n",
       "Donald             28066\n",
       "Obama              28035\n",
       "?                  27492\n",
       "government         26786\n",
       "can                26401\n",
       "He                 26344\n",
       "Republican         25450\n",
       "House              25418\n",
       "In                 25292\n",
       "year               24425\n",
       ",â€                 24296\n",
       "/                  23695\n",
       "told               23330\n",
       "United             22860\n",
       "just               22835\n",
       "We                 22082\n",
       "like               21830\n",
       "state              21276\n",
       "campaign           21257\n",
       "time               20355\n",
       "A                  19937\n",
       "election           19882\n",
       "two                19850\n",
       "president          19578\n",
       "States             19182\n",
       "last               18973\n",
       "But                18857\n",
       "$                  18357\n",
       "country            17936\n",
       "(@                 17884\n",
       "now                17279\n",
       "This               17223\n",
       "new                16820\n",
       "Hillary            16757\n",
       "years              16552\n",
       "former             16440\n",
       "re                 16433\n",
       "first              16337\n",
       "White              16161\n",
       "American           16132\n",
       "even               15892\n",
       "!                  15526\n",
       "made               15432\n",
       "media              15262\n",
       "say                15240\n",
       "political          14885\n",
       ".â€                 14755\n",
       "many               14583\n",
       "get                14457\n",
       "law                14442\n",
       "percent            14392\n",
       "Republicans        14109\n",
       "going              14030\n",
       "Russia             13943\n",
       "make               13937\n",
       "right              13682\n",
       "America            13490\n",
       "back               13477\n",
       "party              13365\n",
       "New                13346\n",
       "called             13170\n",
       "State              13109\n",
       "presidential       13060\n",
       "week               13048\n",
       "since              12896\n",
       "administration     12818\n",
       "support            12788\n",
       "And                12766\n",
       "Senate             12706\n",
       "including          12586\n",
       "know               12491\n",
       "vote               12442\n",
       "way                12440\n",
       "Democratic         12354\n",
       "police             12347\n",
       "think              12269\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from stop_words import get_stop_words\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 100)\n",
    "news_word_counter = Counter()\n",
    "for sent_i in fake_news_sentences:\n",
    "    news_word_counter.update(sent_i)\n",
    "for sent_i in real_news_sentences:\n",
    "    news_word_counter.update(sent_i)\n",
    "news_word_counts = pd.Series(dict(news_word_counter)).sort_values(inplace=False, ascending=False)\n",
    "en_stops = set(get_stop_words('en')) & set(news_word_counts.index)\n",
    "news_word_counts.drop(en_stops, inplace=True)\n",
    "display(news_word_counts.head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = ['Trump', 'President', 'election', 'Republicans', 'Democratic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing word = Trump\n",
      "\tfake news neighbors\n",
      "[('Rubio', 0.7030426263809204), ('Obama', 0.6997063159942627), ('Cruz', 0.6733947992324829), ('he', 0.6496449112892151), ('him', 0.6216100454330444), ('trump', 0.6005026698112488), ('Hillary', 0.5969979166984558), ('Russia', 0.5960808992385864), ('Putin', 0.5954418778419495), ('He', 0.5948378443717957)]\n",
      "\treal news neighbors\n",
      "[('Pence', 0.6959595084190369), ('he', 0.6848228573799133), ('Cruz', 0.6819908022880554), ('Abe', 0.6777504682540894), ('Macron', 0.672048807144165), ('him', 0.6628677845001221), ('Clinton', 0.6577510237693787), ('Rubio', 0.6568259596824646), ('Obama', 0.6540595293045044), ('Duterte', 0.650699257850647)]\n",
      "testing word = President\n",
      "\tfake news neighbors\n",
      "[('Barack', 0.8168875575065613), ('president', 0.8109959959983826), ('administration', 0.7899814248085022), ('Administration', 0.73664391040802), ('presidency', 0.7126862406730652), ('Donald', 0.641764223575592), ('Michelle', 0.60544753074646), ('regime', 0.5730191469192505), ('2016President', 0.5575225353240967), ('nominee', 0.5508920550346375)]\n",
      "\treal news neighbors\n",
      "[('president', 0.6807978749275208), ('Rumsfeld', 0.5728514194488525), ('leader', 0.5449401140213013), ('predecessor', 0.5301105380058289), ('presidency', 0.5064476728439331), ('Presidents', 0.5052238702774048), ('Janot', 0.4890914261341095), ('Governor', 0.4883328974246979), ('Jr', 0.4860992133617401), ('Tusk', 0.48190000653266907)]\n",
      "testing word = election\n",
      "\tfake news neighbors\n",
      "[('elections', 0.7940957546234131), ('primaries', 0.7812632918357849), ('contest', 0.7415914535522461), ('primary', 0.727788507938385), ('race', 0.7144359350204468), ('nomination', 0.7017841935157776), ('debate', 0.7013251781463623), ('landslide', 0.7003783583641052), ('victory', 0.680475115776062), ('candidate', 0.661431074142456)]\n",
      "\treal news neighbors\n",
      "[('elections', 0.8387276530265808), ('race', 0.7429296970367432), ('contest', 0.7061190605163574), ('electoral', 0.6699434518814087), ('candidate', 0.6649948358535767), ('referendum', 0.6485381126403809), ('vote', 0.6472706198692322), ('primary', 0.6435508728027344), ('victory', 0.639729917049408), ('campaign', 0.637935996055603)]\n",
      "testing word = Republicans\n",
      "\tfake news neighbors\n",
      "[('Democrats', 0.9475896954536438), ('conservatives', 0.8750953078269958), ('voters', 0.7377797961235046), ('candidates', 0.734005868434906), ('lawmakers', 0.7294324040412903), ('politicians', 0.7233482003211975), ('parties', 0.7001833319664001), ('GOP', 0.6930691003799438), ('Dems', 0.6926183700561523), ('progressives', 0.6808422803878784)]\n",
      "\treal news neighbors\n",
      "[('Democrats', 0.9482499957084656), ('conservatives', 0.8086115121841431), ('lawmakers', 0.7555219531059265), ('parties', 0.6887307167053223), ('candidates', 0.6762281060218811), ('senators', 0.6744876503944397), ('MPs', 0.669486939907074), ('Conservatives', 0.6613771915435791), ('voters', 0.6518579125404358), ('Catalans', 0.6454824805259705)]\n",
      "testing word = Democratic\n",
      "\tfake news neighbors\n",
      "[('Republican', 0.8929237127304077), ('Democrat', 0.8530702590942383), ('GOP', 0.7768924832344055), ('Tea', 0.7761590480804443), ('Libertarian', 0.7159236073493958), ('republican', 0.6959002017974854), ('Labour', 0.6792230606079102), ('Dance', 0.6680352091789246), ('Vermont', 0.6488345265388489), ('Communist', 0.6222336292266846)]\n",
      "\treal news neighbors\n",
      "[('Republican', 0.8826723098754883), ('Socialist', 0.7737101912498474), ('Conservative', 0.7484926581382751), ('Tea', 0.7340053915977478), ('Janata', 0.6930243372917175), ('Regions', 0.6910569071769714), ('Labour', 0.6855936050415039), ('PRI', 0.6693596243858337), ('AK', 0.6657257080078125), ('Green', 0.6640552282333374)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-152-ff8394035b46>:6: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  print(fake_news_word2vec_embed_model.most_similar(test_word_i, topn=N_neighbors))\n",
      "<ipython-input-152-ff8394035b46>:8: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  print(real_news_word2vec_embed_model.most_similar(test_word_i, topn=N_neighbors))\n"
     ]
    }
   ],
   "source": [
    "## test word2vec first\n",
    "N_neighbors = 10\n",
    "for test_word_i in test_words:\n",
    "    print(f'testing word = {test_word_i}')\n",
    "    print(f'\\tfake news neighbors')\n",
    "    print(fake_news_word2vec_embed_model.most_similar(test_word_i, topn=N_neighbors))\n",
    "    print(f'\\treal news neighbors')\n",
    "    print(real_news_word2vec_embed_model.most_similar(test_word_i, topn=N_neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing word = Trump\n",
      "\tfake news neighbors\n",
      "[('Donald', 0.8976928852032244), ('he', 0.750797318175638), ('President', 0.7224733577899606), ('elect', 0.6950681370557232), ('his', 0.6873461754719997), ('him', 0.6607629129400844), ('presidency', 0.6461921033349649), ('supporter', 0.6430482444622551), ('Q13FOXWATCH', 0.6391587798777106)]\n",
      "\treal news neighbors\n",
      "[('Donald', 0.9018925471531778), ('Obama', 0.7329701551897114), ('Clinton', 0.7253358950095211), ('he', 0.7219566927581803), ('Putin', 0.7197551851813184), ('administration', 0.6996611063749594), ('He', 0.6981073253324681), ('president', 0.6959299640971673), ('elect', 0.6852015437442879)]\n",
      "testing word = President\n",
      "\tfake news neighbors\n",
      "[('Obama', 0.8745549150502164), ('president', 0.8300468244161047), ('Barack', 0.8096793836572981), ('Donald', 0.7728321776884208), ('ObamaE', 0.7725010583428265), ('administration', 0.7640559438864527), ('elect', 0.72963454464097), ('Trump', 0.7224733577899606), ('Putin', 0.6720365624445165)]\n",
      "\treal news neighbors\n",
      "[('Barack', 0.8086528300552979), ('Obama', 0.7646152458376682), ('Xi', 0.7389069232532137), ('elect', 0.7368471446400732), ('president', 0.7302385123936124), ('Vice', 0.7139533290514882), ('Donald', 0.7106481830728736), ('Emmanuel', 0.676188596399093), ('Vladimir', 0.6730685127116153)]\n",
      "testing word = election\n",
      "\tfake news neighbors\n",
      "[('presidential', 0.7869030615179107), ('win', 0.720060544873601), ('won', 0.7132810872509115), ('primary', 0.6993691381721916), ('elections', 0.6968645147282432), ('campaign', 0.6931552945626136), ('2016', 0.6858446734482803), ('race', 0.6676134607769463), ('run', 0.6649361558749709)]\n",
      "\treal news neighbors\n",
      "[('2016', 0.8276092183784373), ('elections', 0.8038909872124897), ('presidential', 0.8021766049652546), ('November', 0.7758173176891402), ('July', 0.6976699113656692), ('race', 0.6882295040152506), ('vote', 0.684555047813963), ('2018', 0.679158655287818), ('September', 0.6789581944865516)]\n",
      "testing word = Republicans\n",
      "\tfake news neighbors\n",
      "[('Democrats', 0.8970723922712085), ('conservatives', 0.8148132327953772), ('Congress', 0.7409676790706138), ('GOP', 0.7393818534989127), ('Republican', 0.731515418262319), ('won', 0.7098147035949577), ('win', 0.6848942420475854), ('still', 0.6770851149583786), ('voters', 0.671580131272387)]\n",
      "\treal news neighbors\n",
      "[('Democrats', 0.9083597890275228), ('lawmakers', 0.8429549312237947), ('both', 0.7712426367402855), ('Congress', 0.7378831039044729), ('senators', 0.7309557134007222), ('Republican', 0.6954619729683196), ('Senate', 0.6881861456624728), ('some', 0.6649588635108648), ('opposed', 0.6541944347503562)]\n",
      "testing word = Democratic\n",
      "\tfake news neighbors\n",
      "[('Republican', 0.8743542358951663), ('Democrat', 0.8346653988277387), ('Party', 0.8245918845324374), ('GOP', 0.8095375670581186), ('presidential', 0.8038427238701031), ('candidates', 0.7607760735703655), ('candidate', 0.7523742575192388), ('primary', 0.7308515892576932), ('party', 0.7101687436907573)]\n",
      "\treal news neighbors\n",
      "[('Republican', 0.8267092941135616), ('candidate', 0.7693512256701805), ('Democrat', 0.7540856714394084), ('Hillary', 0.7160327348592473), ('nominee', 0.7076168317931374), ('rival', 0.6871714778132513), ('Party', 0.683357075767194), ('candidates', 0.6828167151299152), ('presumptive', 0.6783729100416315)]\n"
     ]
    }
   ],
   "source": [
    "## test Glove embeddings\n",
    "N_neighbors = 10\n",
    "for test_word_i in test_words:\n",
    "    print(f'testing word = {test_word_i}')\n",
    "    print(f'\\tfake news neighbors')\n",
    "    print(fake_news_glove_embed_model.most_similar(test_word_i, number=N_neighbors))\n",
    "    print(f'\\treal news neighbors')\n",
    "    print(real_news_glove_embed_model.most_similar(test_word_i, number=N_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see some aspects of potential bias with these test words.\n",
    "\n",
    "For `word2vec`:\n",
    "- `Trump` is associated with almost exclusively Republican politicians in fake news and with a mix of politicians in real news\n",
    "- `President` is associated more with U.S. politics in fake news and more with international politicians in real news\n",
    "- `Democratic` are associated more with U.S. politics in fake news and more with international politics in real news\n",
    "\n",
    "For `Glove`:\n",
    "- `Trump` is associated with himself (and news network? `Q13FOXWATCH`) in fake news and with other presidents in real news\n",
    "- `President` is associated with Trump and Obama in fake news and more with international politicians in real news\n",
    "- `Democratic` is associated with U.S. party politics in both fake and real news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This qualitative analysis helps us understand that some words may indeed have significant divergence in their connotations between the different data sets, while others are more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which words are the most different across the data?\n",
    "\n",
    "We'll measure \"difference\" using the overlap in nearest neighbors (i.e. Jaccard similarity).\n",
    "\n",
    "$$\\text{diff(word1, word2)} = 1 - \\frac{\\text{neighbors(word1)} \\: \\cap \\: \\text{neighbors(word2)}}{\\text{neighbors(word1)} \\cup \\text{neighbors(word2)}}$$\n",
    "\n",
    "A difference of 100% means that the words have no neighbors in common, while a difference of 0% means that the words have identical neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_neighbor_diff(neighbors_1, neighbors_2):\n",
    "    neighbor_intersect = set(neighbors_1) & set(neighbors_2)\n",
    "    neighbor_union = set(neighbors_1) | set(neighbors_2)\n",
    "    neighbor_diff = 1 - len(neighbor_intersect) / len(neighbor_union)\n",
    "    return neighbor_diff\n",
    "def compute_neighbor_diff_model(word, model_1, model_2, N_neighbor, model_type='word2vec'):\n",
    "    if(model_type == 'word2vec'):\n",
    "        neighbors_1, neighbor_scores_1 = zip(*model_1.wv.most_similar(word, topn=N_neighbor))\n",
    "        neighbors_2, neighbor_scores_2 = zip(*model_2.wv.most_similar(word, topn=N_neighbor))\n",
    "    elif(model_type == 'glove'):\n",
    "        neighbors_1, neighbor_scores_1 = zip(*model_1.most_similar(word, number=N_neighbor))\n",
    "        neighbors_2, neighbor_scores_2 = zip(*model_2.most_similar(word, number=N_neighbor))\n",
    "    neighbor_diff = compute_neighbor_diff(neighbors_1, neighbors_2)\n",
    "    return neighbor_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23922 words in word2vec vocab\n"
     ]
    }
   ],
   "source": [
    "# get shared vocabulary\n",
    "shared_word2vec_vocab = list(set(fake_news_word2vec_embed_model.wv.vocab.keys()) & set(real_news_word2vec_embed_model.wv.vocab.keys()))\n",
    "print(f'{len(shared_word2vec_vocab)} words in word2vec vocab')\n",
    "# compute neighbor differences for all valid words\n",
    "model_type = 'word2vec'\n",
    "N_neighbor = 10\n",
    "fake_vs_real_word2vec_neighbor_diffs = list(map(lambda x: compute_neighbor_diff_model(x, fake_news_word2vec_embed_model, real_news_word2vec_embed_model, N_neighbor, model_type=model_type), shared_word2vec_vocab))\n",
    "# add vocabulary as index\n",
    "fake_vs_real_word2vec_neighbor_diffs = pd.Series(fake_vs_real_word2vec_neighbor_diffs, index=shared_word2vec_vocab)\n",
    "fake_vs_real_word2vec_neighbor_diffs.sort_values(inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with most neighbor difference\n",
      "Charlie         1.0\n",
      "placement       1.0\n",
      "installation    1.0\n",
      "stoned          1.0\n",
      "convenience     1.0\n",
      "Federalist      1.0\n",
      "rewards         1.0\n",
      "Pablo           1.0\n",
      "princess        1.0\n",
      "mortgage        1.0\n",
      "fentanyl        1.0\n",
      "rebuked         1.0\n",
      "hog             1.0\n",
      "cantons         1.0\n",
      "installed       1.0\n",
      "systemically    1.0\n",
      "purchases       1.0\n",
      "sideshow        1.0\n",
      "Medium          1.0\n",
      "theme           1.0\n",
      "dtype: float64\n",
      "words with most neighbor similarity\n",
      "Friday       0.181818\n",
      "Sunday       0.181818\n",
      "February     0.181818\n",
      "Their        0.181818\n",
      "two          0.181818\n",
      "November     0.181818\n",
      "their        0.181818\n",
      "Monday       0.181818\n",
      "Thursday     0.181818\n",
      "March        0.181818\n",
      "October      0.181818\n",
      "down         0.181818\n",
      "Tuesday      0.181818\n",
      "December     0.181818\n",
      "July         0.181818\n",
      "January      0.181818\n",
      "Wednesday    0.181818\n",
      "April        0.181818\n",
      "cannot       0.181818\n",
      "15           0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "top_k = 20\n",
    "print('words with most neighbor difference')\n",
    "print(fake_vs_real_word2vec_neighbor_diffs.head(top_k))\n",
    "print('words with most neighbor similarity')\n",
    "print(fake_vs_real_word2vec_neighbor_diffs.tail(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words with the biggest neighbor differences don't seem to be super informative and may reflect topical differences (e.g. fake news tends to discuss `Charlie` more often and therefore has more consistent nearest neighbors).\n",
    "\n",
    "What if we restrict to the top-1000 most frequent words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequent words with most neighbor difference\n",
      "left            1.0\n",
      "process         1.0\n",
      "=               1.0\n",
      "via             1.0\n",
      "talks           1.0\n",
      "fear            1.0\n",
      "!               1.0\n",
      "O               1.0\n",
      "J               1.0\n",
      "News            1.0\n",
      "Barack          1.0\n",
      "face            1.0\n",
      "influence       1.0\n",
      "course          1.0\n",
      "(@              1.0\n",
      "like            1.0\n",
      "image           1.0\n",
      "United          1.0\n",
      "co              1.0\n",
      "host            1.0\n",
      "twitter         1.0\n",
      "comment         1.0\n",
      "continued       1.0\n",
      "hit             1.0\n",
      "Minister        1.0\n",
      "Black           1.0\n",
      "independence    1.0\n",
      "[               1.0\n",
      "&               1.0\n",
      "West            1.0\n",
      "use             1.0\n",
      ".-              1.0\n",
      "reality         1.0\n",
      "'               1.0\n",
      "head            1.0\n",
      "New             1.0\n",
      "yet             1.0\n",
      "Islamic         1.0\n",
      "/               1.0\n",
      "air             1.0\n",
      "Images          1.0\n",
      "(               1.0\n",
      "com             1.0\n",
      "Watch           1.0\n",
      "corruption      1.0\n",
      "*               1.0\n",
      "Johnson         1.0\n",
      "immediately     1.0\n",
      "U               1.0\n",
      ";               1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# only keep the words that are in the word2vec vocab\n",
    "word2vec_vocab_news_word_counts = news_word_counts.loc[(news_word_counts.index & set(shared_word2vec_vocab))].sort_values(inplace=False, ascending=False)\n",
    "top_N_words = word2vec_vocab_news_word_counts.iloc[:1000].index.tolist()\n",
    "top_N_fake_vs_real_word2vec_neighbor_diffs = fake_vs_real_word2vec_neighbor_diffs.loc[top_N_words].sort_values(inplace=False, ascending=False)\n",
    "top_k = 50\n",
    "print('frequent words with most neighbor difference')\n",
    "print(top_N_fake_vs_real_word2vec_neighbor_diffs.head(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! This leaves us with some interesting words to investigate:\n",
    "\n",
    "- `left` (related to politics?)\n",
    "- `Barack`\n",
    "- `twitter`\n",
    "- `Black`\n",
    "- `Islamic`\n",
    "- `corruption`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing word = left\n",
      "\tfake news neighbors\n",
      "[('right', 0.7388289570808411), ('gone', 0.5697188973426819), ('fringe', 0.5553905367851257), ('conservative', 0.5482199192047119), ('liberal', 0.5463820695877075), ('destroyed', 0.5287902355194092), ('pushed', 0.5214146375656128), ('side', 0.5122994780540466), ('Right', 0.5121271014213562), ('shifted', 0.5109315514564514)]\n",
      "\treal news neighbors\n",
      "[('lost', 0.7303896546363831), ('abandoned', 0.7045692205429077), ('entered', 0.685556173324585), ('kept', 0.6793034672737122), ('gained', 0.6761770844459534), ('regained', 0.667672336101532), ('stayed', 0.6657394766807556), ('stuck', 0.6575697660446167), ('started', 0.638910174369812), ('secured', 0.6378580927848816)]\n",
      "testing word = Barack\n",
      "\tfake news neighbors\n",
      "[('President', 0.816887617111206), ('Michelle', 0.7705560922622681), ('Administration', 0.7330065965652466), ('2016President', 0.7039303779602051), ('administration', 0.6872981786727905), ('Ronald', 0.6653918027877808), ('Jeb', 0.6605669260025024), ('Administrations', 0.6391910314559937), ('Hussein', 0.6269696950912476), ('presidency', 0.6213403940200806)]\n",
      "\treal news neighbors\n",
      "[('Andrzej', 0.7358546853065491), ('Faure', 0.7260587811470032), ('Salva', 0.7243014574050903), ('Muhammadu', 0.7189658284187317), ('Buhari', 0.6989701390266418), ('Yoweri', 0.6963873505592346), ('Mauricio', 0.6930809020996094), ('Shavkat', 0.6917094588279724), ('Nicolas', 0.6814447641372681), ('Kiir', 0.6672935485839844)]\n",
      "testing word = twitter\n",
      "\tfake news neighbors\n",
      "[('WTVM', 0.5977560877799988), ('hillaryclinton', 0.5855060815811157), ('100percentfedup', 0.573275089263916), ('BlackHistoryMonth', 0.5666057467460632), ('Philly', 0.5642104148864746), ('AlternativeFacts', 0.5491176843643188), ('au', 0.5414866805076599), ('www', 0.5366619825363159), ('MAGA', 0.5352457761764526), ('TrumpSacrifices', 0.5347346663475037)]\n",
      "\treal news neighbors\n",
      "[('microblog', 0.7735182642936707), ('Weibo', 0.7514196038246155), ('reflections', 0.735605776309967), ('Instagram', 0.7166048884391785), ('crotch', 0.7040818929672241), ('truculent', 0.7000449299812317), ('NB', 0.6996288895606995), ('retort', 0.6991008520126343), ('sarcastically', 0.6903284788131714), ('Col', 0.6898080110549927)]\n",
      "testing word = Black\n",
      "\tfake news neighbors\n",
      "[('Matter', 0.7706215381622314), ('Lives', 0.7336457371711731), ('BLM', 0.6777241826057434), ('Tycoon', 0.6274093985557556), ('rallying', 0.6151938438415527), ('Panthers', 0.6093480587005615), ('Antifa', 0.604564905166626), ('Panther', 0.6015989780426025), ('Hispanic', 0.5822928547859192), ('Odin', 0.5758909583091736)]\n",
      "\treal news neighbors\n",
      "[('Red', 0.7002115845680237), ('Beacon', 0.6662045121192932), ('Charlotte', 0.6553705930709839), ('League', 0.6503832340240479), ('Historical', 0.6475571990013123), ('Town', 0.6459012627601624), ('Conservation', 0.645159900188446), ('Cultural', 0.6422840356826782), ('Square', 0.636823296546936), ('Correspondents', 0.6359671354293823)]\n",
      "testing word = Islamic\n",
      "\tfake news neighbors\n",
      "[('Palestinian', 0.6977366209030151), ('Deep', 0.6950808763504028), ('Islamist', 0.6764666438102722), ('Jewish', 0.627342164516449), ('Muslim', 0.6246275901794434), ('Palestine', 0.6241163015365601), ('BLM', 0.6208215951919556), ('muslim', 0.6138649582862854), ('terrorist', 0.6126998662948608), ('arming', 0.6098479628562927)]\n",
      "\treal news neighbors\n",
      "[('Daesh', 0.6964540481567383), ('Capture', 0.6833562254905701), ('Duma', 0.6767401099205017), ('Kachin', 0.6627154350280762), ('IS', 0.650870680809021), ('Secretaries', 0.6254169344902039), ('ISIS', 0.6207304000854492), ('Sunshine', 0.6155365109443665), ('SDF', 0.6115128397941589), ('Counsellor', 0.6114950180053711)]\n",
      "testing word = corruption\n",
      "\tfake news neighbors\n",
      "[('hacking', 0.7731659412384033), ('deception', 0.7556135654449463), ('ongoing', 0.7095997333526611), ('lawlessness', 0.7042204737663269), ('hacks', 0.7002522945404053), ('investigations', 0.6959912776947021), ('scrutiny', 0.6937267780303955), ('violations', 0.6929147839546204), ('secrecy', 0.6720618009567261), ('avoiding', 0.6667200326919556)]\n",
      "\treal news neighbors\n",
      "[('graft', 0.9250772595405579), ('fraud', 0.7226190567016602), ('criminal', 0.7129549980163574), ('bribery', 0.6976973414421082), ('scandals', 0.6973208785057068), ('alleged', 0.6820821166038513), ('scandal', 0.6791611909866333), ('cronyism', 0.6640135049819946), ('rebellion', 0.6488530039787292), ('crime', 0.6459413766860962)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-154-6200cba35b5d>:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  print(fake_news_word2vec_embed_model.most_similar(word_i, topn=N_neighbors))\n",
      "<ipython-input-154-6200cba35b5d>:9: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  print(real_news_word2vec_embed_model.most_similar(word_i, topn=N_neighbors))\n"
     ]
    }
   ],
   "source": [
    "# print neighbors for all high-difference words\n",
    "high_diff_words = ['left', 'Barack', 'twitter', 'Black', 'Islamic', 'corruption']\n",
    "N_neighbors = 10\n",
    "for word_i in high_diff_words:\n",
    "    print(f'testing word = {word_i}')\n",
    "    print(f'\\tfake news neighbors')\n",
    "    print(fake_news_word2vec_embed_model.most_similar(word_i, topn=N_neighbors))\n",
    "    print(f'\\treal news neighbors')\n",
    "    print(real_news_word2vec_embed_model.most_similar(word_i, topn=N_neighbors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reveals some serious bias going on in the fake news articles.\n",
    "\n",
    "- `left` is more associated with extreme political views in fake news, and more associated with the traditional verb sense in real news\n",
    "- `Barack` is more associated with the Obama administration (and his \"unusual\" name `Hussein`) in fake news, and more associated with world leaders in real news\n",
    "- `twitter` is more associated with \"alternative\" news sources in fake news, and more associated with social media in general in real news\n",
    "- `Black` is more associated with the Black Lives Matter movement and other left-wing movements (`antifa`) in fake news, and more associated with a variety of organizations in real news\n",
    "- `Islamic` is more associated with terrorist and perceived \"radical\" movements in fake news, and more associated with Middle Eastern politics in real news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: visualize?? https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration\n",
    "Now it's time for you to try out some more tests with word embeddings!\n",
    "\n",
    "- Increasing the **window size** when training embeddings can help the embeddings capture more global context (e.g. associating `tomato` with cooking details from the wider sentence context). How would this help capture divides between fake news and real news?\n",
    "- One way to determine the **connotation** of a word in embedding space is to look at its proximity to positive and negative words: e.g. if `Barack` is consistently closer to words like `bad` and `terrible` than to `good` and `nice`. Can you come up with a way to test word connotations using this kind of approach, and determine whether some words have consistently better or worse connotations in fake news articles?\n",
    "- Another useful aspect of word embeddings is their tendency to **cluster** words into general semantic fields, e.g. grouping all politician names near one another. Using the visualization technique from earlier, try to find words that (1) consistently fall into neat clusters and (2) sometimes appear outside of the expected clusters in the data. Which political and organizational words tend to be represented outside of their expected cluster, and why do you think that happens? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## data = cross-cultural data\n",
    "# # collect/save\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "# def load_deception_data_from_dir(data_dir):\n",
    "#     sub_dirs = list(map(lambda x: os.path.join(data_dir, x), os.listdir(data_dir)))\n",
    "#     sub_dirs = list(filter(lambda x: os.path.isdir(x), sub_dirs))\n",
    "#     print(sub_dirs)\n",
    "#     invalid_file_matcher = re.compile('^\\._?')\n",
    "#     sub_dir_data_files = [os.path.join(x, y) for x in sub_dirs for y in os.listdir(x) if invalid_file_matcher.search(y) is None]\n",
    "#     combined_data = []\n",
    "#     for data_file_i in sub_dir_data_files:\n",
    "# #         print(f'processing data = {data_file_i}')\n",
    "#         topic_i, label_i = os.path.basename(data_file_i).split('.')\n",
    "# #         full_data_file_i = os.path.join(data_dir, data_file_i)\n",
    "# #         print()\n",
    "#         data_i = pd.read_csv(data_file_i, sep='\\t', header=None, index_col=False, skip_blank_lines=True)\n",
    "#         data_i.columns = ['id', 'text']\n",
    "#         # keep valid text\n",
    "#         data_i = data_i[data_i.loc[:, 'text'].apply(lambda x: type(x) is str)]\n",
    "# #         print(data_i.head())\n",
    "# #         for x in data_i.loc[:, 'id'].values:\n",
    "# #             print(x)\n",
    "# #             print(x.split('_')[1])\n",
    "#         data_i = data_i.assign(**{\n",
    "#             'id' : data_i.loc[:, 'id'].apply(lambda x: int(x.split('_')[1]))\n",
    "#         })\n",
    "#         data_i = data_i.assign(**{'topic' : topic_i, 'label' : label_i})\n",
    "#         combined_data.append(data_i)\n",
    "#     combined_data = pd.concat(combined_data, axis=0)\n",
    "#     return combined_data\n",
    "# # us_deception_data_dir = 'data/crossCulturalDeception.2014/EnglishUS/'\n",
    "# # print([x for x in os.listdir(us_deception_data_dir) if os.path.isdir(os.path.join(us_deception_data_dir, x))])\n",
    "# us_deception_data = load_deception_data_from_dir(us_deception_data_dir)\n",
    "# display(us_deception_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
